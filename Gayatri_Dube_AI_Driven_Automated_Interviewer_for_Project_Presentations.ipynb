{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j286ZXG56aMS",
        "outputId": "527feb0b-3c72-4aea-8d81-19993e7b6e3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m32.9/32.9 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hdpkg: libjack-jackd2-0:amd64: dependency problems, but removing anyway as you requested:\n",
            " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\n",
            "  Package libjack-jackd2-0:amd64 is to be removed.\n",
            "  Package libjack-0.125 is not installed.\n",
            "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\n",
            " libavdevice58:amd64 depends on libjack-jackd2-0 (>= 1.9.10+20150825) | libjack-0.125; however:\n",
            "  Package libjack-jackd2-0:amd64 is to be removed.\n",
            "  Package libjack-0.125 is not installed.\n",
            "  Package libjack-jackd2-0:amd64 which provides libjack-0.125 is to be removed.\n",
            "\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Removing libjack-jackd2-0:amd64 (1.9.20~dfsg-1) ...\n",
            "Selecting previously unselected package libjack0:amd64.\n",
            "(Reading database ... 117518 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libjack0_1%3a0.125.0-3build2_amd64.deb ...\n",
            "Unpacking libjack0:amd64 (1:0.125.0-3build2) ...\n",
            "Selecting previously unselected package libasound2-dev:amd64.\n",
            "Preparing to unpack .../1-libasound2-dev_1.2.6.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libasound2-dev:amd64 (1.2.6.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libjack-dev.\n",
            "Preparing to unpack .../2-libjack-dev_1%3a0.125.0-3build2_amd64.deb ...\n",
            "Unpacking libjack-dev (1:0.125.0-3build2) ...\n",
            "Selecting previously unselected package libportaudio2:amd64.\n",
            "Preparing to unpack .../3-libportaudio2_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package libportaudiocpp0:amd64.\n",
            "Preparing to unpack .../4-libportaudiocpp0_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Selecting previously unselected package portaudio19-dev:amd64.\n",
            "Preparing to unpack .../5-portaudio19-dev_19.6.0-1.1_amd64.deb ...\n",
            "Unpacking portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Setting up libjack0:amd64 (1:0.125.0-3build2) ...\n",
            "Setting up libjack-dev (1:0.125.0-3build2) ...\n",
            "Setting up libasound2-dev:amd64 (1.2.6.1-1ubuntu1) ...\n",
            "Setting up libportaudio2:amd64 (19.6.0-1.1) ...\n",
            "Setting up libportaudiocpp0:amd64 (19.6.0-1.1) ...\n",
            "Setting up portaudio19-dev:amd64 (19.6.0-1.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.11) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install -q gradio google-generativeai gtts SpeechRecognition pydub\n",
        "!apt-get -qq install -y ffmpeg portaudio19-dev\n",
        "\n",
        "print(\"‚úÖ Step 1 Complete: All dependencies installed!\")\n",
        "print(\"‚û°Ô∏è Next: Run CELL 2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All libraries imported**"
      ],
      "metadata": {
        "id": "9MOclrwfBCZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple\n",
        "from PIL import Image\n",
        "from gtts import gTTS\n",
        "import tempfile\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "print(\"‚úÖ Step 2 Complete: All libraries imported!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6Zh7Z3z6ckY",
        "outputId": "a3cd3c72-7aa4-447c-fbaf-46b401390d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Step 2 Complete: All libraries imported!\n",
            "‚û°Ô∏è Next: Run CELL 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3 Complete: Configuration and Session classes defined!**"
      ],
      "metadata": {
        "id": "Z8rIv3s76lyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    \"\"\"Configuration for API keys\"\"\"\n",
        "    GOOGLE_API_KEY = \"AIzaSyAltXbvQLTFj4CtjQbBAYV-F7qT2iGBwnM\"  # Replace with your key\n",
        "\n",
        "class InterviewSession:\n",
        "    \"\"\"Manages the interview state and conversation\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.conversation_history = []\n",
        "        self.presentation_content = []\n",
        "        self.screen_analysis = []\n",
        "        self.questions_asked = []\n",
        "        self.responses = []\n",
        "        self.evaluation_data = {\n",
        "            'technical_depth': [],\n",
        "            'clarity': [],\n",
        "            'originality': [],\n",
        "            'understanding': []\n",
        "        }\n",
        "        self.current_question_num = 0\n",
        "        self.individual_scores = []\n",
        "        self.asked_topics = set()\n",
        "        self.is_ended = False\n",
        "\n",
        "    def add_screen_content(self, content: str, content_type: str):\n",
        "        \"\"\"Add extracted screen content\"\"\"\n",
        "        self.screen_analysis.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'type': content_type,\n",
        "            'content': content\n",
        "        })\n",
        "\n",
        "    def add_qa_pair(self, question: str, answer: str):\n",
        "        \"\"\"Add question-answer pair\"\"\"\n",
        "        self.questions_asked.append(question)\n",
        "        self.responses.append(answer)\n",
        "        self.conversation_history.append({\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "    def add_evaluation_scores(self, scores: Dict):\n",
        "        \"\"\"Add individual evaluation scores\"\"\"\n",
        "        self.individual_scores.append(scores)\n",
        "        for key in ['technical_depth', 'clarity', 'originality', 'understanding']:\n",
        "            if key in scores:\n",
        "                self.evaluation_data[key].append(scores[key])\n",
        "\n",
        "print(\"‚úÖ Step 3 Complete: Configuration and Session classes defined!\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuhVvOFO6kcK",
        "outputId": "92c21867-e2d5-4697-b6b8-4f4ca5b5fcb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Step 3 Complete: Configuration and Session classes defined!\n",
            "‚û°Ô∏è Next: Run CELL 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AI Interviewer class defined!**"
      ],
      "metadata": {
        "id": "HK9FhtK-A1KV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AIInterviewer:\n",
        "\n",
        "    def __init__(self, api_key: str):\n",
        "        import google.generativeai as genai\n",
        "\n",
        "        self.api_key = api_key\n",
        "        genai.configure(api_key=api_key)\n",
        "\n",
        "        # Use correct model name\n",
        "        try:\n",
        "            self.model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "        except:\n",
        "            try:\n",
        "                self.model = genai.GenerativeModel('gemini-pro')\n",
        "            except Exception as e:\n",
        "                raise Exception(f\"Failed to initialize model. Error: {e}\")\n",
        "\n",
        "        self.session = InterviewSession()\n",
        "\n",
        "    def text_to_speech(self, text: str) -> str:\n",
        "        \"\"\"Convert text to speech and return audio file path\"\"\"\n",
        "        try:\n",
        "            clean_text = re.sub(r'[*#_`]', '', text)\n",
        "            clean_text = re.sub(r'Question \\d+:', '', clean_text)\n",
        "            clean_text = clean_text.strip()\n",
        "\n",
        "            if not clean_text:\n",
        "                return None\n",
        "\n",
        "            tts = gTTS(text=clean_text, lang='en', slow=False)\n",
        "            temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.mp3')\n",
        "            tts.save(temp_file.name)\n",
        "\n",
        "            return temp_file.name\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"TTS Error: {e}\")\n",
        "            return None\n",
        "\n",
        "    def analyze_screen_content(self, image_data, ocr_text: str = \"\") -> Tuple[str, str]:\n",
        "        \"\"\"Analyze screen content and return text + audio\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"Analyze this presentation screen/slide briefly.\n",
        "\n",
        "{f'Additional context: {ocr_text}' if ocr_text else ''}\n",
        "\n",
        "In 2-3 sentences, identify:\n",
        "1. Main topic/technology shown\n",
        "2. Key technical component or feature\n",
        "3. One specific aspect worth questioning\n",
        "\n",
        "Be concise and specific.\"\"\"\n",
        "\n",
        "        try:\n",
        "            if image_data is not None:\n",
        "                img = Image.fromarray(image_data)\n",
        "                response = self.model.generate_content([prompt, img])\n",
        "            else:\n",
        "                response = self.model.generate_content(prompt)\n",
        "\n",
        "            analysis = response.text\n",
        "            self.session.add_screen_content(analysis, \"screen_analysis\")\n",
        "\n",
        "            audio_path = self.text_to_speech(f\"I can see your presentation. {analysis}\")\n",
        "\n",
        "            return analysis, audio_path\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error analyzing screen: {str(e)}\"\n",
        "            print(error_msg)\n",
        "            return error_msg, None\n",
        "\n",
        "    def generate_question(self, context: str, previous_qa: List[Dict]) -> Tuple[str, str]:\n",
        "        \"\"\"Generate DIVERSE question and return text + audio\"\"\"\n",
        "\n",
        "        screen_context = \"\\n\".join([\n",
        "            f\"Screen: {s['content'][:100]}\"\n",
        "            for s in self.session.screen_analysis[-2:]\n",
        "        ])\n",
        "\n",
        "        previous_questions = \"\\n\".join([\n",
        "            f\"Already asked: {q}\"\n",
        "            for q in self.session.questions_asked\n",
        "        ])\n",
        "\n",
        "        qa_context = \"\"\n",
        "        if previous_qa:\n",
        "            last_qa = previous_qa[-1]\n",
        "            qa_context = f\"Last Q: {last_qa['question']}\\nLast A: {last_qa['answer'][:150]}\"\n",
        "\n",
        "        question_types = [\n",
        "            \"implementation details and technical choices\",\n",
        "            \"architecture and design patterns\",\n",
        "            \"challenges faced and solutions\",\n",
        "            \"performance and optimization\",\n",
        "            \"testing and debugging approach\",\n",
        "            \"scalability and future improvements\",\n",
        "            \"alternative approaches considered\",\n",
        "            \"integration with other components\"\n",
        "        ]\n",
        "\n",
        "        available_types = [t for t in question_types if t not in self.session.asked_topics]\n",
        "        if not available_types:\n",
        "            self.session.asked_topics.clear()\n",
        "            available_types = question_types\n",
        "\n",
        "        chosen_type = random.choice(available_types)\n",
        "        self.session.asked_topics.add(chosen_type)\n",
        "\n",
        "        prompt = f\"\"\"You are a technical interviewer. Generate ONE NEW question about {chosen_type}.\n",
        "\n",
        "PRESENTATION:\n",
        "{screen_context}\n",
        "\n",
        "{qa_context}\n",
        "\n",
        "ALREADY ASKED (DO NOT REPEAT):\n",
        "{previous_questions}\n",
        "\n",
        "Generate a DIFFERENT question focusing on {chosen_type}.\n",
        "\n",
        "Requirements:\n",
        "- Must be COMPLETELY DIFFERENT from previous questions\n",
        "- Focus on {chosen_type}\n",
        "- Be specific and technical\n",
        "- Require detailed explanation\n",
        "- Start with \"How\", \"Why\", \"What\", or \"Describe\"\n",
        "\n",
        "Return ONLY the question.\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            question = response.text.strip()\n",
        "            question = question.replace('\"', '').replace(\"Question:\", \"\").strip()\n",
        "            question = re.sub(r'^\\d+\\.\\s*', '', question)\n",
        "\n",
        "            self.session.current_question_num += 1\n",
        "\n",
        "            audio_path = self.text_to_speech(f\"Question {self.session.current_question_num}. {question}\")\n",
        "\n",
        "            return question, audio_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating question: {e}\")\n",
        "            fallback = f\"What {chosen_type} did you implement in this project?\"\n",
        "            self.session.current_question_num += 1\n",
        "            return fallback, self.text_to_speech(fallback)\n",
        "\n",
        "    def evaluate_response(self, question: str, answer: str, screen_context: str) -> Dict:\n",
        "        \"\"\"Evaluate student response\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"Evaluate this technical answer briefly.\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "ANSWER: {answer}\n",
        "\n",
        "Rate 1-10 for each:\n",
        "- technical_depth: Implementation understanding\n",
        "- clarity: Clear explanation\n",
        "- originality: Creative approach\n",
        "- understanding: Concept grasp\n",
        "\n",
        "Return ONLY JSON (no markdown):\n",
        "{{\"technical_depth\": 7, \"clarity\": 8, \"originality\": 6, \"understanding\": 7, \"brief_feedback\": \"Good detail on implementation.\"}}\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            response_text = response.text.strip()\n",
        "\n",
        "            response_text = re.sub(r'```json\\s*', '', response_text)\n",
        "            response_text = re.sub(r'```\\s*', '', response_text)\n",
        "\n",
        "            json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
        "            if json_match:\n",
        "                eval_data = json.loads(json_match.group())\n",
        "                self.session.add_evaluation_scores(eval_data)\n",
        "                return eval_data\n",
        "            else:\n",
        "                default = {\n",
        "                    \"technical_depth\": 6, \"clarity\": 6, \"originality\": 6,\n",
        "                    \"understanding\": 6, \"brief_feedback\": \"Answer recorded.\"\n",
        "                }\n",
        "                self.session.add_evaluation_scores(default)\n",
        "                return default\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error evaluating response: {e}\")\n",
        "            default = {\n",
        "                \"technical_depth\": 6, \"clarity\": 6, \"originality\": 6,\n",
        "                \"understanding\": 6, \"brief_feedback\": \"Answer recorded.\"\n",
        "            }\n",
        "            self.session.add_evaluation_scores(default)\n",
        "            return default\n",
        "\n",
        "    def generate_final_report(self) -> Tuple[str, str]:\n",
        "        \"\"\"Generate comprehensive report with audio\"\"\"\n",
        "\n",
        "        scores = {}\n",
        "        for key in ['technical_depth', 'clarity', 'originality', 'understanding']:\n",
        "            if self.session.evaluation_data[key]:\n",
        "                scores[key] = np.mean(self.session.evaluation_data[key])\n",
        "            else:\n",
        "                scores[key] = 6.0\n",
        "\n",
        "        avg_score = sum(scores.values()) / 4\n",
        "\n",
        "        if avg_score >= 9: grade = \"A+\"\n",
        "        elif avg_score >= 8.5: grade = \"A\"\n",
        "        elif avg_score >= 7.5: grade = \"B+\"\n",
        "        elif avg_score >= 7: grade = \"B\"\n",
        "        elif avg_score >= 6: grade = \"C+\"\n",
        "        elif avg_score >= 5: grade = \"C\"\n",
        "        else: grade = \"D\"\n",
        "\n",
        "        qa_summary = \"\\n\\n\".join([\n",
        "            f\"Q{i+1}: {qa['question']}\\nA{i+1}: {qa['answer'][:200]}...\"\n",
        "            for i, qa in enumerate(self.session.conversation_history)\n",
        "        ])\n",
        "\n",
        "        prompt = f\"\"\"Generate evaluation report.\n",
        "\n",
        "INTERVIEW: {len(self.session.conversation_history)} questions answered\n",
        "SCORES: Tech:{scores['technical_depth']:.1f}, Clarity:{scores['clarity']:.1f}, Originality:{scores['originality']:.1f}, Understanding:{scores['understanding']:.1f}\n",
        "AVERAGE: {avg_score:.1f}, GRADE: {grade}\n",
        "\n",
        "Generate:\n",
        "1. OVERALL ASSESSMENT (2 sentences)\n",
        "2. STRENGTHS (3 points)\n",
        "3. IMPROVEMENTS (3 points)\n",
        "4. RECOMMENDATIONS (3 points)\"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            report_body = response.text\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating report body: {e}\")\n",
        "            report_body = \"\"\"\n",
        "OVERALL ASSESSMENT\n",
        "Your interview demonstrated solid understanding of the project fundamentals.\n",
        "Continue developing your technical communication skills.\n",
        "\n",
        "STRENGTHS\n",
        "‚Ä¢ Answered all questions thoughtfully\n",
        "‚Ä¢ Showed engagement with the material\n",
        "‚Ä¢ Demonstrated project knowledge\n",
        "\n",
        "AREAS FOR IMPROVEMENT\n",
        "‚Ä¢ Provide more technical depth in explanations\n",
        "‚Ä¢ Include specific implementation details\n",
        "‚Ä¢ Discuss challenges and solutions\n",
        "\n",
        "RECOMMENDATIONS\n",
        "‚Ä¢ Practice explaining technical concepts clearly\n",
        "‚Ä¢ Prepare examples of your decision-making process\n",
        "‚Ä¢ Study design patterns and best practices\n",
        "\"\"\"\n",
        "\n",
        "        header = f\"\"\"\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë          FINAL INTERVIEW EVALUATION REPORT                   ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üìä FINAL SCORES\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Criterion           ‚îÇ Score  ‚îÇ Visualization              ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Technical Depth     ‚îÇ {scores['technical_depth']:>5.1f} ‚îÇ {'‚ñà' * int(scores['technical_depth'])}{'‚ñë' * (10-int(scores['technical_depth']))} ‚îÇ\n",
        "‚îÇ Clarity             ‚îÇ {scores['clarity']:>5.1f} ‚îÇ {'‚ñà' * int(scores['clarity'])}{'‚ñë' * (10-int(scores['clarity']))} ‚îÇ\n",
        "‚îÇ Originality         ‚îÇ {scores['originality']:>5.1f} ‚îÇ {'‚ñà' * int(scores['originality'])}{'‚ñë' * (10-int(scores['originality']))} ‚îÇ\n",
        "‚îÇ Understanding       ‚îÇ {scores['understanding']:>5.1f} ‚îÇ {'‚ñà' * int(scores['understanding'])}{'‚ñë' * (10-int(scores['understanding']))} ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ OVERALL AVERAGE     ‚îÇ {avg_score:>5.1f} ‚îÇ {'‚ñà' * int(avg_score)}{'‚ñë' * (10-int(avg_score))} ‚îÇ\n",
        "‚îÇ FINAL GRADE         ‚îÇ   {grade:>3}  ‚îÇ                            ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "üìà QUESTION-BY-QUESTION BREAKDOWN\n",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\n",
        "\"\"\"\n",
        "\n",
        "        for i, score_data in enumerate(self.session.individual_scores):\n",
        "            q_avg = (score_data.get('technical_depth', 6) +\n",
        "                    score_data.get('clarity', 6) +\n",
        "                    score_data.get('originality', 6) +\n",
        "                    score_data.get('understanding', 6)) / 4\n",
        "\n",
        "            header += f\"\\nQ{i+1}: {q_avg:.1f}/10 - {score_data.get('brief_feedback', 'N/A')}\\n\"\n",
        "\n",
        "        header += \"\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\\n\"\n",
        "\n",
        "        full_report = header + report_body\n",
        "\n",
        "        audio_text = f\"\"\"Interview complete. Your final grade is {grade} with an average score of {avg_score:.1f} out of 10.\n",
        "Your strongest area was {max(scores, key=scores.get).replace('_', ' ')} with {max(scores.values()):.1f}.\n",
        "Check the detailed report for feedback.\"\"\"\n",
        "\n",
        "        audio_path = self.text_to_speech(audio_text)\n",
        "\n",
        "        return full_report, audio_path\n",
        "\n",
        "print(\"‚úÖ Step 4 Complete: AI Interviewer class defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAs9LuTH6p_G",
        "outputId": "d04a6408-94bc-48ac-a7a2-8ef2b90b8d2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Step 4 Complete: AI Interviewer class defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradio Application**"
      ],
      "metadata": {
        "id": "M9PlPh6-AsoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "\n",
        "    interviewer = None\n",
        "\n",
        "    def initialize_interview(api_key):\n",
        "        nonlocal interviewer\n",
        "        if not api_key:\n",
        "            return \"‚ùå Please provide a valid Google API key\", \"\", None, gr.update(visible=False)\n",
        "\n",
        "        try:\n",
        "            interviewer = AIInterviewer(api_key)\n",
        "            welcome_audio = interviewer.text_to_speech(\n",
        "                \"Welcome to your AI interview. Upload a screenshot or describe your project to begin.\"\n",
        "            )\n",
        "            return (\n",
        "                \"‚úÖ Interview initialized!\\n\\nüì∏ Upload screenshot OR describe your project.\",\n",
        "                \"\",\n",
        "                welcome_audio,\n",
        "                gr.update(visible=False)\n",
        "            )\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error initializing: {str(e)}\\n\\nTry getting a new API key from https://makersuite.google.com/app/apikey\", \"\", None, gr.update(visible=False)\n",
        "\n",
        "    def process_screen_and_transcription(image, transcription, api_key):\n",
        "        \"\"\"Process screen and generate first question\"\"\"\n",
        "        nonlocal interviewer\n",
        "\n",
        "        if interviewer is None:\n",
        "            try:\n",
        "                interviewer = AIInterviewer(api_key)\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå Initialize interview first. Error: {e}\", \"\", None, gr.update(visible=False)\n",
        "\n",
        "        if image is None and not transcription:\n",
        "            return \"‚ö†Ô∏è Provide screenshot or description.\", \"\", None, gr.update(visible=False)\n",
        "\n",
        "        try:\n",
        "            if image is not None:\n",
        "                analysis, _ = interviewer.analyze_screen_content(image, transcription or \"\")\n",
        "            else:\n",
        "                analysis = f\"Student: {transcription}\"\n",
        "                interviewer.session.add_screen_content(transcription, \"speech\")\n",
        "\n",
        "            question, audio = interviewer.generate_question(\n",
        "                context=analysis,\n",
        "                previous_qa=interviewer.session.conversation_history\n",
        "            )\n",
        "\n",
        "            output = f\"üìä **Analysis:**\\n{analysis}\\n\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\\n‚ùì **Question {interviewer.session.current_question_num}:**\\n{question}\"\n",
        "\n",
        "            return output, question, audio, gr.update(visible=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error processing: {str(e)}\", \"\", None, gr.update(visible=False)\n",
        "\n",
        "    def submit_answer_text(current_question, student_answer, api_key):\n",
        "        \"\"\"Process TEXT answer\"\"\"\n",
        "        return process_answer(current_question, student_answer, api_key)\n",
        "\n",
        "    def submit_answer_voice(current_question, audio_input, api_key):\n",
        "        \"\"\"Process VOICE answer\"\"\"\n",
        "        if audio_input is None:\n",
        "            return \"‚ö†Ô∏è No audio recorded.\", current_question, \"\", None, gr.update(visible=True)\n",
        "\n",
        "        try:\n",
        "            import speech_recognition as sr\n",
        "            recognizer = sr.Recognizer()\n",
        "\n",
        "            with sr.AudioFile(audio_input) as source:\n",
        "                audio_data = recognizer.record(source)\n",
        "                text = recognizer.recognize_google(audio_data)\n",
        "\n",
        "            return process_answer(current_question, text, api_key)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ö†Ô∏è Could not process audio: {str(e)}\\n\\nPlease try typing your answer instead.\", current_question, \"\", None, gr.update(visible=True)\n",
        "\n",
        "    def process_answer(current_question, student_answer, api_key):\n",
        "        \"\"\"Process answer and generate next question\"\"\"\n",
        "        nonlocal interviewer\n",
        "\n",
        "        if interviewer is None:\n",
        "            return \"‚ùå Initialize interview first.\", \"\", \"\", None, gr.update(visible=False)\n",
        "\n",
        "        if not student_answer:\n",
        "            return \"‚ö†Ô∏è Please provide an answer.\", current_question, \"\", None, gr.update(visible=True)\n",
        "\n",
        "        try:\n",
        "            interviewer.session.add_qa_pair(current_question, student_answer)\n",
        "\n",
        "            screen_context = interviewer.session.screen_analysis[-1]['content'] if interviewer.session.screen_analysis else \"\"\n",
        "            evaluation = interviewer.evaluate_response(current_question, student_answer, screen_context)\n",
        "\n",
        "            if interviewer.session.is_ended or len(interviewer.session.responses) >= 5:\n",
        "                report, audio = interviewer.generate_final_report()\n",
        "\n",
        "                feedback = f\"‚úÖ Final answer recorded!\\n\\nüí¨ {evaluation.get('brief_feedback', '')}\\n\\nüéì Interview complete!\"\n",
        "\n",
        "                return feedback, \"\", report, audio, gr.update(visible=False)\n",
        "\n",
        "            else:\n",
        "                next_question, audio = interviewer.generate_question(\n",
        "                    context=student_answer,\n",
        "                    previous_qa=interviewer.session.conversation_history\n",
        "                )\n",
        "\n",
        "                feedback = f\"‚úÖ Recorded!\\n\\nüí¨ {evaluation.get('brief_feedback', '')}\\n\\n‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\\n\\n‚ùì **Question {interviewer.session.current_question_num}:**\\n{next_question}\"\n",
        "\n",
        "                return feedback, next_question, \"\", audio, gr.update(visible=True)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error processing answer: {str(e)}\", current_question, \"\", None, gr.update(visible=True)\n",
        "\n",
        "    def end_interview_now(api_key):\n",
        "        \"\"\"End interview early and generate report\"\"\"\n",
        "        nonlocal interviewer\n",
        "\n",
        "        if interviewer is None:\n",
        "            return \"‚ùå No active interview to end.\", None, gr.update(visible=False)\n",
        "\n",
        "        if len(interviewer.session.responses) < 1:\n",
        "            return \"‚ö†Ô∏è Please answer at least 1 question before ending the interview.\", None, gr.update(visible=False)\n",
        "\n",
        "        try:\n",
        "            interviewer.session.is_ended = True\n",
        "            report, audio = interviewer.generate_final_report()\n",
        "\n",
        "            return report, audio, gr.update(visible=False)\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Error generating report: {str(e)}\", None, gr.update(visible=False)\n",
        "\n",
        "    with gr.Blocks(title=\"AI Voice Interviewer\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # üé§ AI Voice Interviewer - Answer with TEXT or VOICE!\n",
        "        ### Interactive Project Presentation Interview\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### üîë Setup\")\n",
        "                api_key_input = gr.Textbox(\n",
        "                    label=\"Google API Key\",\n",
        "                    placeholder=\"Paste key...\",\n",
        "                    type=\"password\",\n",
        "                    value=Config.GOOGLE_API_KEY\n",
        "                )\n",
        "                init_btn = gr.Button(\"üöÄ Start Interview\", variant=\"primary\", size=\"lg\")\n",
        "                status_output = gr.Textbox(label=\"Status\", lines=2)\n",
        "\n",
        "                gr.Markdown(\"### üéß AI Voice\")\n",
        "                audio_output = gr.Audio(label=\"AI Response\", autoplay=True)\n",
        "\n",
        "                gr.Markdown(\"### üì∏ Your Presentation\")\n",
        "                screen_image = gr.Image(label=\"Screenshot\", type=\"numpy\")\n",
        "                transcription_input = gr.Textbox(\n",
        "                    label=\"Or Describe\",\n",
        "                    placeholder=\"Explain your project...\",\n",
        "                    lines=3\n",
        "                )\n",
        "                process_btn = gr.Button(\"üîç Submit & Get Question\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### üí¨ Interview\")\n",
        "                interview_output = gr.Textbox(label=\"AI Interviewer\", lines=8)\n",
        "\n",
        "                current_q = gr.Textbox(label=\"Current Question\", visible=False)\n",
        "\n",
        "                gr.Markdown(\"### ‚úçÔ∏è Answer (Choose One)\")\n",
        "\n",
        "                with gr.Tab(\"üìù Type Answer\"):\n",
        "                    answer_text = gr.Textbox(\n",
        "                        label=\"Type Your Answer\",\n",
        "                        placeholder=\"Explain in detail...\",\n",
        "                        lines=5\n",
        "                    )\n",
        "                    submit_text_btn = gr.Button(\"üì§ Submit Text Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                with gr.Tab(\"üé§ Speak Answer\"):\n",
        "                    answer_voice = gr.Audio(\n",
        "                        label=\"Record Your Answer\",\n",
        "                        sources=[\"microphone\"],\n",
        "                        type=\"filepath\"\n",
        "                    )\n",
        "                    submit_voice_btn = gr.Button(\"üì§ Submit Voice Answer\", variant=\"primary\", size=\"lg\")\n",
        "\n",
        "                end_btn = gr.Button(\n",
        "                    \"üèÅ End Interview & Get Report Now\",\n",
        "                    variant=\"stop\",\n",
        "                    size=\"lg\",\n",
        "                    visible=False\n",
        "                )\n",
        "\n",
        "                gr.Markdown(\"### üìä Report\")\n",
        "                final_report = gr.Textbox(label=\"Final Evaluation\", lines=15)\n",
        "\n",
        "        gr.Markdown(\"\"\"\n",
        "        ---\n",
        "        ### üìã How to Use:\n",
        "\n",
        "        1. **Get FREE API Key**: [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
        "        2. **Start Interview**: Click \"Start Interview\"\n",
        "        3. **Present**: Upload screenshot OR describe project\n",
        "        4. **Answer**: Choose TEXT or VOICE tab\n",
        "           - üìù **Type**: Write your answer and click \"Submit Text Answer\"\n",
        "           - üé§ **Speak**: Click microphone, speak, then \"Submit Voice Answer\"\n",
        "        5. **End Early**: Click \"üèÅ End Interview & Get Report Now\" after answering at least 1 question\n",
        "        6. **Auto-Complete**: Interview auto-ends after 5 questions\n",
        "\n",
        "        ### üí° Tips:\n",
        "        - Each question will be DIFFERENT (no repetition!)\n",
        "        - Use microphone for hands-free answering\n",
        "        - Or type for detailed technical responses\n",
        "        - Show code, diagrams, UI screenshots\n",
        "        - You can end the interview early to get your report\n",
        "\n",
        "        ### üîß Troubleshooting:\n",
        "        - **404 Error**: Get a new API key from Google AI Studio\n",
        "        - **Voice not working**: Use text input instead\n",
        "        - **No audio**: Check your speakers/headphones\n",
        "        \"\"\")\n",
        "\n",
        "        init_btn.click(\n",
        "            fn=initialize_interview,\n",
        "            inputs=[api_key_input],\n",
        "            outputs=[status_output, final_report, audio_output, end_btn]\n",
        "        )\n",
        "\n",
        "        process_btn.click(\n",
        "            fn=process_screen_and_transcription,\n",
        "            inputs=[screen_image, transcription_input, api_key_input],\n",
        "            outputs=[interview_output, current_q, audio_output, end_btn]\n",
        "        )\n",
        "\n",
        "        submit_text_btn.click(\n",
        "            fn=submit_answer_text,\n",
        "            inputs=[current_q, answer_text, api_key_input],\n",
        "            outputs=[interview_output, current_q, final_report, audio_output, end_btn]\n",
        "        )\n",
        "\n",
        "        submit_voice_btn.click(\n",
        "            fn=submit_answer_voice,\n",
        "            inputs=[current_q, answer_voice, api_key_input],\n",
        "            outputs=[interview_output, current_q, final_report, audio_output, end_btn]\n",
        "        )\n",
        "\n",
        "        end_btn.click(\n",
        "            fn=end_interview_now,\n",
        "            inputs=[api_key_input],\n",
        "            outputs=[final_report, audio_output, end_btn]\n",
        "        )\n",
        "\n",
        "    return demo\n",
        "\n",
        "# Launch the application\n",
        "print(\"üöÄ Launching AI Voice Interviewer...\")\n",
        "demo = create_gradio_interface()\n",
        "demo.launch(share=True, debug=True)\n",
        "print(\"\\n‚úÖ Step 5 Complete: Application is running!\")\n",
        "print(\"üì± Click the link above to access your AI interviewer!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rrlav6io6zka",
        "outputId": "613acc03-f04b-49b9-9722-8cccc8bd9949"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Launching AI Voice Interviewer...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-4013828521.py:135: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(title=\"AI Voice Interviewer\", theme=gr.themes.Soft()) as demo:\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://fde662d20b5c9b9a85.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://fde662d20b5c9b9a85.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/google/colab/_import_hooks/_hook_injector.py:55: FutureWarning: \n",
            "\n",
            "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
            "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
            "See README for more details:\n",
            "\n",
            "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
            "\n",
            "  loader.exec_module(module)\n",
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 609.02ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error generating question: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 279.63ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error evaluating response: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 280.31ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error generating question: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 229.36ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error evaluating response: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 305.79ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error generating question: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 228.97ms\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error generating report body: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ePRMjlYd63Qm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}